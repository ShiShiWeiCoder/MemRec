# use BERT/chatGLM to encode the multilevel memory enhanced knowledge generated by LLM
import os
import json

import torch
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel

from utils import save_json, get_paragraph_representation

# Mac 设备配置：优先使用 MPS（Apple Silicon），否则使用 CPU
if torch.backends.mps.is_available():
    device = 'mps'
    print("Using MPS (Apple Silicon GPU)")
elif torch.cuda.is_available():
    device = 'cuda'
    print("Using CUDA GPU")
else:
    device = 'cpu'
    print("Using CPU")

# 移除离线模式设置，使用 Hugging Face 在线模型
# os.environ['TRANSFORMERS_OFFLINE'] = '1'

def load_data(path):
    res = []
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        for id, value in data.items():
            res.append([id, value['prompt'], value['ans']])
    return res


def get_history_text(data_path):
    """处理用户历史知识文本"""
    raw_data = load_data(data_path)
    idx_list, hist_text = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        pure_hist = prompt[::-1].split(';', 1)[-1][::-1]
        hist_text.append(pure_hist + '. ' + answer)
        idx_list.append(idx)
    return idx_list, hist_text


def get_item_text(data_path):
    """处理物品知识文本"""
    raw_data = load_data(data_path)
    idx_list, text_list = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        text_list.append(answer)
        idx_list.append(idx)
    return idx_list, text_list


def get_memory_analysis_text(data_path):
    """处理多级记忆分析知识文本"""
    raw_data = load_data(data_path)
    idx_list, text_list = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        # 对于多级记忆分析知识，我们使用完整的答案作为文本
        text_list.append(answer)
        idx_list.append(idx)
    return idx_list, text_list


def get_sensory_memory_text(data_path):
    """处理感觉记忆知识文本"""
    raw_data = load_data(data_path)
    idx_list, text_list = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        # 对于感觉记忆，我们关注即时响应部分
        text_list.append(answer)
        idx_list.append(idx)
    return idx_list, text_list


def get_working_memory_text(data_path):
    """处理工作记忆知识文本"""
    raw_data = load_data(data_path)
    idx_list, text_list = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        # 对于工作记忆，我们关注当前任务相关部分
        text_list.append(answer)
        idx_list.append(idx)
    return idx_list, text_list


def get_long_term_memory_text(data_path):
    """处理长期记忆知识文本"""
    raw_data = load_data(data_path)
    idx_list, text_list = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        # 对于长期记忆，我们关注职业发展相关部分
        text_list.append(answer)
        idx_list.append(idx)
    return idx_list, text_list


def get_text_data_loader_multilevel_memory(data_path, batch_size):
    """获取多级记忆增强知识的数据加载器"""
    # 加载多级记忆增强的用户知识
    user_memory_path = os.path.join(data_path, 'user_multilevel_memory.klg')
    hist_idxes, history = get_history_text(user_memory_path)
    print('Multilevel Memory User Knowledge:', history[1] if len(history) > 1 else "Empty", 'hist len', len(history))
    
    # 加载多级记忆增强的物品知识
    item_memory_path = os.path.join(data_path, 'item_multilevel_memory.klg')
    item_idxes, items = get_item_text(item_memory_path)
    print('Multilevel Memory Item Knowledge:', items[1] if len(items) > 1 else "Empty", 'item len', len(items))
    
    # 加载多级记忆分析知识
    memory_analysis_path = os.path.join(data_path, 'memory_analysis.klg')
    analysis_idxes, analysis_texts = get_memory_analysis_text(memory_analysis_path)
    print('Multilevel Memory Analysis Knowledge:', analysis_texts[1] if len(analysis_texts) > 1 else "Empty", 'analysis len', len(analysis_texts))

    # 可选：加载具体的感觉记忆、工作记忆、长期记忆知识
    sensory_memory_path = os.path.join(data_path, 'sensory_memory.klg')
    working_memory_path = os.path.join(data_path, 'working_memory.klg')
    long_term_memory_path = os.path.join(data_path, 'long_term_memory.klg')
    
    sensory_idxes, sensory_texts = [], []
    working_idxes, working_texts = [], []
    longterm_idxes, longterm_texts = [], []
    
    if os.path.exists(sensory_memory_path):
        sensory_idxes, sensory_texts = get_sensory_memory_text(sensory_memory_path)
        print('Sensory Memory Knowledge:', sensory_texts[1] if len(sensory_texts) > 1 else "Empty", 'sensory len', len(sensory_texts))
    
    if os.path.exists(working_memory_path):
        working_idxes, working_texts = get_working_memory_text(working_memory_path)
        print('Working Memory Knowledge:', working_texts[1] if len(working_texts) > 1 else "Empty", 'working len', len(working_texts))
    
    if os.path.exists(long_term_memory_path):
        longterm_idxes, longterm_texts = get_long_term_memory_text(long_term_memory_path)
        print('Long-term Memory Knowledge:', longterm_texts[1] if len(longterm_texts) > 1 else "Empty", 'longterm len', len(longterm_texts))

    history_loader = DataLoader(history, batch_size, shuffle=False)
    item_loader = DataLoader(items, batch_size, shuffle=False)
    analysis_loader = DataLoader(analysis_texts, batch_size, shuffle=False)
    
    # 可选的多级记忆加载器
    sensory_loader = DataLoader(sensory_texts, batch_size, shuffle=False) if sensory_texts else None
    working_loader = DataLoader(working_texts, batch_size, shuffle=False) if working_texts else None
    longterm_loader = DataLoader(longterm_texts, batch_size, shuffle=False) if longterm_texts else None
    
    return (history_loader, hist_idxes, 
            item_loader, item_idxes, 
            analysis_loader, analysis_idxes,
            sensory_loader, sensory_idxes,
            working_loader, working_idxes,
            longterm_loader, longterm_idxes)


def remap_item(item_idxes, item_vec):
    item_vec_map = {}
    for idx, vec in zip(item_idxes, item_vec):
        item_vec_map[idx] = vec
    return item_vec_map


def inference(model, tokenizer, dataloader, model_name, aggregate_type):
    pred_list = []
    model.eval()
    with torch.no_grad():
        for x in tqdm(dataloader):
            # 清理内存缓存（适用于所有设备）
            if device == 'cuda':
                torch.cuda.empty_cache()
            elif device == 'mps':
                torch.mps.empty_cache()
            
            # 统一使用BERT样式的处理（移除ChatGLM特殊处理）
            x = tokenizer(x, padding=True, truncation=True, max_length=512, 
                         return_tensors="pt", return_attention_mask=True).to(device)
            mask = x['attention_mask']
            outputs = model(**x, output_hidden_states=True, return_dict=True)
            
            pred = get_paragraph_representation(outputs, mask, aggregate_type)
            pred_list.extend(pred.tolist())
    return pred_list


def main(knowledge_path, data_path, model_name, batch_size, aggregate_type):
    # 获取多级记忆增强知识的数据加载器
    (hist_loader, hist_idxes, 
     item_loader, item_idxes, 
     analysis_loader, analysis_idxes,
     sensory_loader, sensory_idxes,
     working_loader, working_idxes,
     longterm_loader, longterm_idxes) = get_text_data_loader_multilevel_memory(knowledge_path, batch_size)

    # 支持多种BERT模型
    if model_name == 'bert-base-uncased':
        checkpoint = 'bert-base-uncased'
        hidden_size = 768
    elif model_name == 'bert-base-cased':
        checkpoint = 'bert-base-cased'
        hidden_size = 768
    elif model_name == 'bert-large-uncased':
        checkpoint = 'bert-large-uncased'
        hidden_size = 1024
    elif model_name == 'bert-chinese':
        checkpoint = 'bert-base-chinese'
        hidden_size = 768
    elif model_name == 'bert-multilingual':
        checkpoint = 'bert-base-multilingual-cased'
        hidden_size = 768
    elif model_name == 'roberta-chinese':
        checkpoint = 'hfl/chinese-roberta-wwm-ext'
        hidden_size = 768
    elif model_name == 'bert-large-chinese':
        checkpoint = 'hfl/chinese-bert-wwm-ext'
        hidden_size = 768
    elif model_name == 'macbert':
        checkpoint = 'hfl/chinese-macbert-base'
        hidden_size = 768
    else:
        # 默认使用 bert-base-uncased
        checkpoint = 'bert-base-uncased'
        hidden_size = 768
        print(f"未识别的模型名称 {model_name}，使用默认的 bert-base-uncased")

    print(f"Loading model from Hugging Face: {checkpoint}")
    print(f"Using device: {device}")
    
    # 清理内存缓存
    if device == 'cuda':
        torch.cuda.empty_cache()
    elif device == 'mps':
        torch.mps.empty_cache()
    
    # 加载 tokenizer 和模型
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    
    print("Loading model...")
    # 根据设备类型配置模型
    if device == 'cpu':
        model = AutoModel.from_pretrained(checkpoint, torch_dtype=torch.float32).to(device)
    elif device == 'mps':
        model = AutoModel.from_pretrained(checkpoint, torch_dtype=torch.float16).to(device)
    else:
        model = AutoModel.from_pretrained(checkpoint).half().to(device)

    print("Encoding multilevel memory enhanced knowledge...")
    
    # 编码物品知识
    print("Encoding item knowledge...")
    item_vec = inference(model, tokenizer, item_loader, model_name, aggregate_type)
    item_vec_dict = remap_item(item_idxes, item_vec)
    
    # 编码用户历史知识
    print("Encoding user history knowledge...")
    hist_vec = inference(model, tokenizer, hist_loader, model_name, aggregate_type)
    hist_vec_dict = remap_item(hist_idxes, hist_vec)
    
    # 编码多级记忆分析知识
    print("Encoding multilevel memory analysis knowledge...")
    analysis_vec = inference(model, tokenizer, analysis_loader, model_name, aggregate_type)
    analysis_vec_dict = remap_item(analysis_idxes, analysis_vec)

    # 编码具体的多级记忆知识（如果存在）
    sensory_vec_dict, working_vec_dict, longterm_vec_dict = {}, {}, {}
    
    if sensory_loader is not None:
        print("Encoding sensory memory knowledge...")
        sensory_vec = inference(model, tokenizer, sensory_loader, model_name, aggregate_type)
        sensory_vec_dict = remap_item(sensory_idxes, sensory_vec)
    
    if working_loader is not None:
        print("Encoding working memory knowledge...")
        working_vec = inference(model, tokenizer, working_loader, model_name, aggregate_type)
        working_vec_dict = remap_item(working_idxes, working_vec)
        
    if longterm_loader is not None:
        print("Encoding long-term memory knowledge...")
        longterm_vec = inference(model, tokenizer, longterm_loader, model_name, aggregate_type)
        longterm_vec_dict = remap_item(longterm_idxes, longterm_vec)

    # 保存编码结果，使用 _multilevel_memory 后缀区分多级记忆增强版本
    save_json(item_vec_dict, os.path.join(data_path, '{}_{}_augment_multilevel_memory.item'.format(model_name, aggregate_type)))
    save_json(hist_vec_dict, os.path.join(data_path, '{}_{}_augment_multilevel_memory.hist'.format(model_name, aggregate_type)))
    save_json(analysis_vec_dict, os.path.join(data_path, '{}_{}_augment_multilevel_memory.analysis'.format(model_name, aggregate_type)))
    
    # 保存具体的多级记忆向量（如果存在）
    if sensory_vec_dict:
        save_json(sensory_vec_dict, os.path.join(data_path, '{}_{}_augment_sensory_memory.vector'.format(model_name, aggregate_type)))
    if working_vec_dict:
        save_json(working_vec_dict, os.path.join(data_path, '{}_{}_augment_working_memory.vector'.format(model_name, aggregate_type)))
    if longterm_vec_dict:
        save_json(longterm_vec_dict, os.path.join(data_path, '{}_{}_augment_longterm_memory.vector'.format(model_name, aggregate_type)))

    # 更新统计信息
    stat_path = os.path.join(data_path, 'stat.json')
    with open(stat_path, 'r') as f:
        stat = json.load(f)

    # BERT 系列模型的隐藏层维度通常为 768
    stat['dense_dim'] = hidden_size
    # 添加多级记忆增强知识的统计信息
    stat['multilevel_memory_knowledge'] = {
        'user_memory_count': len(hist_vec_dict),
        'item_memory_count': len(item_vec_dict),
        'memory_analysis_count': len(analysis_vec_dict),
        'sensory_memory_count': len(sensory_vec_dict),
        'working_memory_count': len(working_vec_dict),
        'longterm_memory_count': len(longterm_vec_dict),
        'model_used': model_name,
        'checkpoint_used': checkpoint,
        'device_used': device,
        'hidden_size': hidden_size
    }
    
    with open(stat_path, 'w') as f:
        stat = json.dumps(stat, indent=2)
        f.write(stat)

    print(f"Multilevel memory enhanced knowledge encoding completed!")
    print(f"- User knowledge vectors: {len(hist_vec_dict)}")
    print(f"- Item knowledge vectors: {len(item_vec_dict)}")
    print(f"- Memory analysis vectors: {len(analysis_vec_dict)}")
    print(f"- Sensory memory vectors: {len(sensory_vec_dict)}")
    print(f"- Working memory vectors: {len(working_vec_dict)}")
    print(f"- Long-term memory vectors: {len(longterm_vec_dict)}")
    print(f"- Model used: {model_name} ({checkpoint})")
    print(f"- Device used: {device}")
    print(f"- Hidden size: {hidden_size}")


if __name__ == '__main__':
    DATA_DIR = 'data/'
    DATA_SET_NAME = 'mooc'
    # 使用多级记忆增强的知识目录
    KLG_PATH = os.path.join(DATA_DIR, DATA_SET_NAME, 'knowledge_multilevel_memory')
    DATA_PATH = os.path.join(DATA_DIR, DATA_SET_NAME, 'proc_data')
    
    # 使用 bert-base-uncased 模型
    MODEL_NAME = 'bert-base-uncased'  # 可选: 'bert-base-uncased', 'bert-base-cased', 'bert-large-uncased', 'bert-chinese', 'bert-multilingual', 'roberta-chinese', 'macbert'
    AGGREGATE_TYPE = 'avg'  # last, avg, wavg, cls, ...
    
    # BERT模型可以使用更大的batch size
    BATCH_SIZE = 1 if device == 'cpu' else 8
    
    print(f"Processing multilevel memory enhanced knowledge from: {KLG_PATH}")
    print(f"Output will be saved to: {DATA_PATH}")
    print(f"Using model: {MODEL_NAME} with aggregation: {AGGREGATE_TYPE}")
    print(f"Device: {device}, Batch size: {BATCH_SIZE}")
    
    main(KLG_PATH, DATA_PATH, MODEL_NAME, BATCH_SIZE, AGGREGATE_TYPE) 